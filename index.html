<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PREP-Eval Pre-registration and REporting Protocol for AI Evaluations</title>
</head>

<body>
  <h1>PREP-Eval Pre-registration and REporting Protocol for AI Evaluations</h1>

  <p>
    Evaluation is an integral part of the development cycle of any AI system. As AI grows more sophisticated and general, evaluations are becoming increasingly complex and broad in scope, requiring larger multidisciplinary teams, longer timescales and more elaborate evaluation tools and techniques. Moreover, the opportunity to deploy these general-purpose AI systems across various commercial and public-sector contexts and the potential risks associated with these deployments has created a burgeoning need for non-expert groups to evaluate the suitability and safety of these models for use in different contexts. Surprisingly, though, despite the growing focus on evaluations, there is no established protocol or methodology for conducting AI evaluations. Here we aim to address this gap by presenting the “Pre-registration and REporting Protocol for AI Evaluations” (PREP-Eval), a step-by-step guide for planning and conducting AI evaluations ​​that complements existing transparency tools such as model cards and evaluation factsheets. We draw on insights from analogous practices in fields such as software testing, data mining, and psychology, and incorporate a pre-registration requirement that facilitates the documentation and justification of deviations from the original plan, helping to identify questionable research practices such as selective reporting. Our protocol is designed to support a wide range of stakeholders, including third-party evaluators, oversight bodies, and newcomers to the field, but it is particularly valuable for small and medium-sized research or industry teams that are developing new AI tools or integrating existing models into novel applications and may lack established evaluation pipelines. We demonstrate the application of PREP-Eval across six diverse use cases and anticipate that PREP-Eval will be further consolidated and improved through iterative feedback and collaboration with the broad AI community.
  </p>

  <h2>Resumen</h2>
  <p>
    Escribí acá el resumen un poco más largo de tu trabajo.
    Podés usar varios párrafos si querés.
  </p>

  <h2>PDF</h2>
  <p>
    <a href="mi_paper.pdf" target="_blank">
      Descargar o ver el PDF
    </a>
  </p>
</body>
</html>
