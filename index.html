<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>PRE-EVAL</title>

  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Arial, sans-serif;
      background-color: #ffffff;
      color: #222;
      margin: 0;
      padding: 0;
    }

    .container {
      max-width: 900px;
      margin: 60px auto;
      padding: 0 20px;
      text-align: center;
    }

    h1 {
      font-size: 42px;
      font-weight: 600;
      margin-bottom: 20px;
    }

    .authors {
      font-size: 20px;
      margin-bottom: 14px;
    }

    .authors a {
      color: #1a73e8;
      text-decoration: none;
    }

    .authors a:hover {
      text-decoration: underline;
    }

    .affiliations {
  font-size: 18px;
  color: #555;
  max-width: 900px;
  margin: 0 auto 30px auto;
  line-height: 1.6;
}
.affiliation {
  display: inline;
  margin-right: 18px;
  white-space: normal;
}

    .buttons {
      margin: 30px 0;
    }

    .button {
      display: inline-block;
      margin: 6px;
      padding: 10px 18px;
      border-radius: 20px;
      background-color: #111;
      color: white;
      text-decoration: none;
      font-size: 15px;
    }

    .button:hover {
      background-color: #333;
    }

    .section {
      text-align: left;
      margin-top: 50px;
    }

    .section h2 {
      font-size: 26px;
      margin-bottom: 10px;
      text-align: center;
    }

    .section p {
      font-size: 17px;
      line-height: 1.6;
    }

  .abstract {
  background-color: #f5f5f5;
  padding: 40px 30px;
  margin-top: 50px;
  border-radius: 8px;
  text-align: center;
}

.abstract h2 {
  font-size: 26px;
  margin-bottom: 15px;
  color: #000;
}

.abstract p {
  font-size: 17px;
  line-height: 1.6;
  max-width: 900px;
  margin: 0 auto;
  text-align: justify;
}

.abstract-image {
  display: block;
  max-width: 900px;
  width: 100%;
  margin: 30px auto 0 auto;
}

  .abstract .figure-caption {
  font-size: 12.5px;
  color: #666;
  text-align: justify;
  margin-top: 6px;
}

    .protocol-table {
  width: 100%;
  border-collapse: collapse;
  margin-top: 30px;
  font-size: 16px;
}

.protocol-table td {
  padding: 12px 14px;
  vertical-align: top;
}

.protocol-table tr {
  border-bottom: 1px solid #ddd;
}

.protocol-table .step {
  width: 32%;
  font-weight: 600;
}

.protocol-table .description {
  width: 68%;
}

.protocol-table .phase td {
  background-color: #f5f5f5;
  font-weight: 600;
  text-align: left;
}

.challenge-card {
  background-color: #f5f5f5;
  padding: 20px;
  border-radius: 8px;
}

.challenge-grid {
  display: grid;
  grid-template-columns: repeat(3, 1fr);
  gap: 24px;
  max-width: 1200px;
  margin: 0 auto;
}

.challenge-row-center {
  grid-column: 1 / -1;
  display: grid;
  grid-template-columns: repeat(2, 1fr);
  gap: 24px;
  max-width: 800px;
  margin: 0 auto;
}

    .final-points {
  max-width: 900px;
  margin: 15px auto 20px auto;
  padding-left: 20px;
  font-size: 16px;
  line-height: 1.6;
}

.final-points li {
  margin-bottom: 8px;
}
    
  </style>
</head>

<body>

  <div class="container">

    <h1>PREP-Eval Pre-registration and REporting Protocol for AI Evaluations</h1>

    <div class="authors">
      <a href="https://it.linkedin.com/in/maria-victoria-carro-2409821b6/en" target="_blank">Mar√≠a Victoria Carro</a><sup>1,2</sup>,
      <a href="https://ryanburnell.com" target="_blank">Ryan Burnell</a><sup>3</sup>,
      <a href="https://cmougan.eu" target="_blank">Carlos Mougan</a><sup>4</sup>
      <a href="https://ankareuel.com/research" target="_blank">Anka Reuel</a><sup>5</sup>
      <a href="https://schellaert.org" target="_blank">Wout Schellaert</a><sup>4</sup>
      <a href="https://www.olawalesalaudeen.com" target="_blank">Olawale Elijah Salaudeen</a><sup>6</sup>
      <a href="https://lexzhou.github.io" target="_blank">Lexin Zhou</a><sup>7</sup>
      <a href="https://www.prpaskov.com" target="_blank">Patricia Paskov</a><sup>8</sup>
      <a href="https://www.linkedin.com/in/tonycohn/" target="_blank">Anthony G Cohn</a><sup>9</sup>
      <a href="http://josephorallo.webs.upv.es" target="_blank">Jose Hernandez-Orallo</a><sup>10,11</sup>
    </div>

    <div class="affiliations">
      <span class="affiliation"><sup>1</sup>Universit√† degli Studi di Genova, Italy</span>
      <span class="affiliation"><sup>2</sup>FAIR, IALAB UBA, University of Buenos Aires, Argentina</span>
      <span class="affiliation"><sup>3</sup>The Alan Turing Institute, United Kingdom</span>
      <span class="affiliation"><sup>4</sup>AI Office - European Commission, European Union</span>
      <span class="affiliation"><sup>5</sup>Stanford University, United States</span>
      <span class="affiliation"><sup>6</sup>Massachusetts Institute of Technology, United States</span>
      <span class="affiliation"><sup>7</sup>Princeton University, United States</span>
      <span class="affiliation"><sup>8</sup>Oxford Martin AI Governance Initiative, United Kingdom</span>
      <span class="affiliation"><sup>9</sup>University of Leeds, United Kingdom</span>
      <span class="affiliation"><sup>10</sup>Cambridge University, United Kingdom</span>
      <span class="affiliation"><sup>11</sup>Universitat Polit√®cnica de Val√®ncia, Spain</span>
    </div>

    <div class="buttons">
      <a class="button" href="mi_paper.pdf" target="_blank">üìÑ Paper</a>
    </div>

    <div class="section abstract">
  <h2>Abstract</h2>
  <p>
       Evaluation is an integral part of the development cycle of any AI system. Despite the growing focus on them, there is no established protocol or methodology for conducting AI evaluations. Here we aim to address this gap by presenting the ‚ÄúPre-registration and REporting Protocol for AI Evaluations‚Äù (PREP-Eval), a step-by-step guide for planning and conducting AI evaluations ‚Äã‚Äãthat complements existing transparency tools such as model cards and evaluation factsheets. We draw on insights from analogous practices in fields such as software testing, data mining, and psychology, and incorporate a pre-registration requirement that facilitates the documentation and justification of deviations from the original plan, helping to identify questionable research practices such as selective reporting. Our protocol is designed to support a wide range of stakeholders, including third-party evaluators, oversight bodies, and newcomers to the field, but it is particularly valuable for small and medium-sized research or industry teams that are developing new AI tools or integrating existing models into novel applications and may lack established evaluation pipelines.
      </p>
      <img src="protocol.jpg" alt="PREP-Eval overview" class="abstract-image">

      <p class="figure-caption">
  PREP-Eval lifecycle. Each stage comes with its own substages and documentation. The protocol emphasises the relevance of the first stages (1 to 3), and pre-registration serves several purposes, such as taking these three first stages seriously, before rushing into the other stages and ensuring that the influence of the analysis on an eventual revision of goals is transparent.
</p>
      
    </div>

    <div class="section">
  <h2>The Need for an AI Evaluation Protocol</h2>

  <div class="challenge-grid">
    <div class="challenge-card">
      <h3>Evaluations not well designed or insufficiently scoped</h3>
      <p>Evaluation projects are inefficient, unfocused, ineffective or opaque, lacking standard project-management practices.</p>
    </div>

    <div class="challenge-card">
      <h3>Over-emphasis on pre-release evaluation</h3>
      <p>Evaluations are focused on capability demonstrations and stylised benchmark tasks, creating blind spots that emerge only under realistic use, or sustained interactions over time.</p>
    </div>

    <div class="challenge-card">
      <h3>Evaluation bias due to non-systematic methodology</h3>
      <p>Evaluations are not trusted. Results could be misinterpreted, providing a misleading picture of system capabilities and limitations.</p>
    </div>

    <div class="challenge-row-center">
    <div class="challenge-card">
      <h3>Insufficient process documentation</h3>
      <p>Lack of coordination and difficulty for scaling up cooperative evaluations, especially in big or changing teams.</p>
    </div>

    <div class="challenge-card">
      <h3>Absence of standardisation</h3>
      <p>Difficulty in sharing the evaluation information with other stakeholders and policymakers, for auditing and regulation..</p>
    </div>
  </div>
</div>

    <div class="section">
  <h2>PREP-Eval Protocol</h2>

  <table class="protocol-table">
    <tr class="phase">
      <td colspan="2"><strong>Phase 1: Goals and Objectives</strong></td>
    </tr>

    <tr>
      <td class="step">1.1 Determine project purpose</td>
      <td class="description">
        Describe the relevant background to the evaluation project, including the terminology,
        project goals, and success criteria.
      </td>
    </tr>

    <tr>
      <td class="step">1.2 Determine technical objectives</td>
      <td class="description">
        Identify and justify the targets of the evaluation, e.g., an AI system or a new evaluation
        method, and describe the success criteria in terms of metrics and uncertainty.
      </td>
    </tr>

    <tr>
      <td class="step">1.3 Situation assessment</td>
      <td class="description">
        Develop an inventory of resources, identify requirements and constraints, anticipate risks
        and contingencies, and assess current understanding of the evaluation targets.
      </td>
    </tr>

    <tr class="phase">
      <td colspan="2"><strong>Phase 2: Evaluation Design</strong></td>
    </tr>

    <tr>
      <td class="step">2.1 Identify potential evaluation methods</td>
      <td class="description">
        Review current evaluation methods, assess maturity and adoption, and monitor emerging
        approaches such as red teaming or human evaluations.
      </td>
    </tr>

    <tr>
      <td class="step">2.2 Selection of evaluation methods</td>
      <td class="description">
        Select an evaluation method and rigorously justify your choice. If no suitable methods
        exist, design or build new ones.
      </td>
    </tr>

    <tr>
      <td class="step">2.3 Analysis specification</td>
      <td class="description">
        Decide and justify how evaluation data will be analysed and what estimators will be
        produced, including metrics and error analysis.
      </td>
    </tr>

        <tr class="phase">
      <td colspan="2"><strong>Phase 3: Project Plan</strong></td>
    </tr>

    <tr>
      <td class="step">3.1. Create a project plan</td>
      <td class="description">
        Draft an initial project plan. This may include the major stages of the evaluation process, a realistic timeline, the resources required, the expected outputs and de- liverables of the project, and any other relevant information gathered during the previous planning phases. Distribute the plan for review and input and consolidate it into a final version.
      </td>
    </tr>

    <tr>
      <td class="step">3.2. Pre-register evaluation</td>
      <td class="description">
        Submit a ‚Äúpre-registration‚Äù of the protocol to a time-stamped repository for potential feedback. This pre-registration, covering up to Phase 3.2, should be complemented later alongside project outputs at the end of phase 6.
      </td>
    </tr>

        <tr class="phase">
      <td colspan="2"><strong>Phase 4: Data Collection</strong></td>
    </tr>

    <tr>
      <td class="step">4.1. Experimental setup, annotations and pilots</td>
      <td class="description">
        Verify data quality and integrity (e.g., if existing datasets will be used, ensure they are accessible), run experimental samples, determine the annotation setup, develop filters and classifiers, and conduct pilot tests. If issues are identified, adjust the protocol as needed and document all changes.
      </td>
    </tr>

    <tr>
      <td class="step">4.2. Full data collection</td>
      <td class="description">
        Run full experiment and obtain full data from the AI system, verifying that the data collected follows the pre-defined sampling strategy.
      </td>
    </tr>

    <tr>
      <td class="step">4.3. Data preparation</td>
      <td class="description">
        Clean, format and organize evaluation data. Verify data quality.
      </td>
    </tr>

            <tr class="phase">
      <td colspan="2"><strong>Phase 5: Data Analysis</strong></td>
    </tr>

    <tr>
      <td class="step">5.1. Initial data exploration</td>
      <td class="description">
        Preliminary exploration of evaluation data, including variation across task features. Identify unexpected patterns and adjust the analysis plan.
      </td>
    </tr>

    <tr>
      <td class="step">5.2. Conduct planned analysis</td>
      <td class="description">
        Perform analyses according to analysis plan: aggregate data and calculate summary statistics/metrics, build performance breakdowns, calculate inferential statistics and build prediction models.
      </td>
    </tr>

    <tr>
      <td class="step">5.3. Assess and refine analysis</td>
      <td class="description">
        To ensure the robustness and interpretability of the analyses quantify uncertainty, test the assumptions behind the analytical methods and inspect any unusual results.
      </td>
    </tr>

                <tr class="phase">
      <td colspan="2"><strong>Phase 6: Conclusions and Review</strong></td>
    </tr>

    <tr>
      <td class="step">6.1. Draw conclusions</td>
      <td class="description">
        Synthesize the analytical findings to derive conclusions about the evaluation tar- get(s), considering the limitations of the process.
      </td>
    </tr>

    <tr>
      <td class="step">6.2. Review evaluation process</td>
      <td class="description">
        Examine what aspects of the process worked effectively and which did not, recom- mend improvements for future evaluations and describe the project legacy.
      </td>
    </tr>

    <tr>
      <td class="step">6.3. Determine next steps</td>
      <td class="description">
        Define how and where to communicate results. Decide on next steps (e.g., additional training, further evaluation, deployment).
      </td>
    </tr>

     <tr>
      <td class="step">6.4. Complete the registration</td>
      <td class="description">
        Write the final report. Document and explain any deviations from the pre-registered plan. Submit the final report to ensure transparency and reproducibility of the evaluation effort.
      </td>
    </tr>

  </table>
</div>

      <div class="section">
  <h2>Implementation and Future Steps</h2>

  <p>
    To illustrate how the protocol is applied, we implement it across several evaluation scenarios, which are presented in the appendix of the paper:
  </p>

  <ul class="final-points">
    <li>Red-teaming GPT-3 to find prompts with a high rate of false statements.</li>
    <li>Evaluating Interactional Fairness in Multi-Agent LLM-Based Systems.</li>
    <li>Evaluating the performance of an LLM-based customer service agent.</li>
    <li>A meta-evaluation for diversity and coverage of test cases.</li>
    <li>Evaluating the capabilities of LLMs that incorporate metacognition.</li>
    <li>Choosing between two AI systems, in a face recognition domain.</li>
  </ul>

  <p>
    We plan to associate the protocol with a repository of sample evaluations, building from existing repositories of AI cases and evaluation, for practitioners to take inspiration from.
    
    We anticipate that PREP-Eval will continue to evolve in response to feedback from the broader community, and in particular from its users. We expect the protocol to be refined as it is applied to a wider range of use cases and as additional implementation details are specified. 
  </p>
</div>



  </div>

</body>
</html>
