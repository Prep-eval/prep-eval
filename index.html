<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>PRE-EVAL</title>

  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Arial, sans-serif;
      background-color: #ffffff;
      color: #222;
      margin: 0;
      padding: 0;
    }

    .container {
      max-width: 900px;
      margin: 60px auto;
      padding: 0 20px;
      text-align: center;
    }

    h1 {
      font-size: 42px;
      font-weight: 600;
      margin-bottom: 20px;
    }

    .authors {
      font-size: 18px;
      margin-bottom: 10px;
    }

    .authors a {
      color: #1a73e8;
      text-decoration: none;
    }

    .authors a:hover {
      text-decoration: underline;
    }

    .affiliations {
      font-size: 16px;
      color: #555;
      margin-bottom: 30px;
    }

    .buttons {
      margin: 30px 0;
    }

    .button {
      display: inline-block;
      margin: 6px;
      padding: 10px 18px;
      border-radius: 20px;
      background-color: #111;
      color: white;
      text-decoration: none;
      font-size: 15px;
    }

    .button:hover {
      background-color: #333;
    }

    .section {
      text-align: left;
      margin-top: 50px;
    }

    .section h2 {
      font-size: 26px;
      margin-bottom: 10px;
    }

    .section p {
      font-size: 17px;
      line-height: 1.6;
    }
  </style>
</head>

<body>

  <div class="container">

    <h1>PREP-Eval Pre-registration and REporting Protocol for AI Evaluations</h1>

    <div class="authors">
      <a href="https://linkedin.com/in/autor1" target="_blank">Mar√≠a Victoria Carro</a><sup>1</sup>,
      <a href="https://autor2.com" target="_blank">Ryan Burnell</a><sup>2</sup>,
      <a href="https://linkedin.com/in/autor3" target="_blank">Carlos Mougan</a><sup>1</sup>
      <a href="https://linkedin.com/in/autor3" target="_blank">Anka Reuel</a><sup>1</sup>
      <a href="https://linkedin.com/in/autor3" target="_blank">Wout Schellaert</a><sup>1</sup>
      <a href="https://linkedin.com/in/autor3" target="_blank">Olawale Elijah Salaudeen</a><sup>1</sup>
      <a href="https://linkedin.com/in/autor3" target="_blank">Lexin Zhou</a><sup>1</sup>
      <a href="https://linkedin.com/in/autor3" target="_blank">Patricia Paskov</a><sup>1</sup>
      <a href="https://linkedin.com/in/autor3" target="_blank">Anthony G Cohn</a><sup>1</sup>
      <a href="https://linkedin.com/in/autor3" target="_blank">Jose Hernandez-Orallo</a><sup>1</sup>
    </div>

    <div class="affiliations">
      <sup>1</sup>Universidad / Instituci√≥n A<br>
      <sup>2</sup>Universidad / Instituci√≥n B
    </div>

    <div class="buttons">
      <a class="button" href="pre-eval.pdf" target="_blank">üìÑ Paper</a>
      <a class="button" href="#" target="_blank">üíª Code</a>
      <a class="button" href="#" target="_blank">üìä Dataset</a>
    </div>

    <div class="section">
      <h2>Abstract</h2>
      <p>
        Evaluation is an integral part of the development cycle of any AI system. As AI grows more sophisticated and general, evaluations are becoming increasingly complex and broad in scope, requiring larger multidisciplinary teams, longer timescales and more elaborate evaluation tools and techniques. Moreover, the opportunity to deploy these general-purpose AI systems across various commercial and public-sector contexts and the potential risks associated with these deployments has created a burgeoning need for non-expert groups to evaluate the suitability and safety of these models for use in different contexts. Surprisingly, though, despite the growing focus on evaluations, there is no established protocol or methodology for conducting AI evaluations. Here we aim to address this gap by presenting the ‚ÄúPre-registration and REporting Protocol for AI Evaluations‚Äù (PREP-Eval), a step-by-step guide for planning and conducting AI evaluations ‚Äã‚Äãthat complements existing transparency tools such as model cards and evaluation factsheets. We draw on insights from analogous practices in fields such as software testing, data mining, and psychology, and incorporate a pre-registration requirement that facilitates the documentation and justification of deviations from the original plan, helping to identify questionable research practices such as selective reporting. Our protocol is designed to support a wide range of stakeholders, including third-party evaluators, oversight bodies, and newcomers to the field, but it is particularly valuable for small and medium-sized research or industry teams that are developing new AI tools or integrating existing models into novel applications and may lack established evaluation pipelines. We demonstrate the application of PREP-Eval across six diverse use cases and anticipate that PREP-Eval will be further consolidated and improved through iterative feedback and collaboration with the broad AI community.
      </p>
    </div>

  </div>

</body>
</html>
